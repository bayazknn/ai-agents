{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ef83e1df-6674-4a39-a3bb-752d368dd14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_text = open(\"pdf.txt\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e96f89dd-c079-4f37-bb6d-cd0950fff2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENVIRONMENT CHECK ===\n",
      "‚úÖ GOOGLE_API_KEY found\n",
      "‚úÖ Connected to Google AI (56 models available)\n",
      "=== TESTING CHAIN CREATION ===\n",
      "üîß Starting cached chain creation...\n",
      "‚úÖ API key found\n",
      "‚úÖ Google AI configured\n",
      "‚úÖ PDF text length: 164095 characters\n",
      "üîÑ Creating cached content...\n",
      "‚ùå Caching failed: 429 TotalCachedContentStorageTokensPerModelFreeTier limit exceeded for model gemini-2.0-flash: limit=0, requested=20825\n",
      "üîÑ Falling back to non-cached version...\n",
      "üîÑ Creating simple non-cached chain...\n",
      "‚úÖ Simple chain created!\n",
      "‚úÖ Chain created without cache\n",
      "üîÑ Testing chain...\n",
      "‚úÖ Chain test successful!\n",
      "Response: Please provide me with the paper you would like me to analyze. I need the text of the paper to determine its main topic....\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "def create_cached_chain(pdf_text: str, system_prompt: str):\n",
    "    \"\"\"\n",
    "    Create a cached chain for Gemini - simplified version with better error handling\n",
    "    \"\"\"\n",
    "    print(\"üîß Starting cached chain creation...\")\n",
    "    \n",
    "    # Step 1: Check API key\n",
    "    api_key = \"AIzaSyBlKnbwJp5o-P59gidOhp2UUYw1yW9cxyc\"\n",
    "    if not api_key:\n",
    "        print(\"‚ùå No GOOGLE_API_KEY found in environment\")\n",
    "        return None\n",
    "    print(\"‚úÖ API key found\")\n",
    "    \n",
    "    # Step 2: Configure Google AI\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ Google AI configured\")\n",
    "    \n",
    "    # Step 3: Validate inputs\n",
    "    if not pdf_text or len(pdf_text.strip()) < 10:\n",
    "        print(\"‚ùå PDF text is too short or empty\")\n",
    "        return None\n",
    "    print(f\"‚úÖ PDF text length: {len(pdf_text)} characters\")\n",
    "    \n",
    "    # Step 4: Try to create cached content\n",
    "    try:\n",
    "        print(\"üîÑ Creating cached content...\")\n",
    "        \n",
    "        # Truncate text if too long (Gemini has limits)\n",
    "        max_chars = 100000  # Adjust based on your needs\n",
    "        truncated_text = pdf_text[:max_chars] if len(pdf_text) > max_chars else pdf_text\n",
    "        \n",
    "        cached_content = genai.caching.CachedContent.create(\n",
    "            model=\"models/gemini-2.0-flash\",  # Use a model that definitely supports caching\n",
    "            display_name=\"arxiv_paper_cache\",\n",
    "            system_instruction=system_prompt,\n",
    "            contents=[{\n",
    "                \"role\": \"user\",\n",
    "                \"parts\": [{\"text\": f\"Research paper content:\\n\\n{truncated_text}\"}]\n",
    "            }],\n",
    "        )\n",
    "        print(f\"‚úÖ Cached content created: {cached_content.name}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Caching failed: {str(e)}\")\n",
    "        print(\"üîÑ Falling back to non-cached version...\")\n",
    "        return create_simple_chain(system_prompt)\n",
    "    \n",
    "    # Step 5: Create LLM with cache\n",
    "    try:\n",
    "        print(\"üîÑ Creating LLM with cache...\")\n",
    "        gemini_llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",  # Match the cached model family\n",
    "            temperature=0.1,\n",
    "            model_kwargs={\n",
    "                \"cached_content\": cached_content.name,\n",
    "            }\n",
    "        )\n",
    "        print(\"‚úÖ LLM created with cache\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå LLM creation failed: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 6: Create prompt and chain\n",
    "    try:\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        cached_chain = prompt | gemini_llm\n",
    "        print(\"‚úÖ Chain created successfully!\")\n",
    "        \n",
    "        return (cached_chain, cached_content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Chain creation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def create_simple_chain(system_prompt: str):\n",
    "    \"\"\"Fallback: Create a simple non-cached chain\"\"\"\n",
    "    print(\"üîÑ Creating simple non-cached chain...\")\n",
    "    \n",
    "    try:\n",
    "        gemini_llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-2.0-flash\",\n",
    "            temperature=0.1,\n",
    "        )\n",
    "        \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),  \n",
    "            (\"human\", \"{input}\")\n",
    "        ])\n",
    "        \n",
    "        simple_chain = prompt | gemini_llm\n",
    "        print(\"‚úÖ Simple chain created!\")\n",
    "        \n",
    "        return (simple_chain, None)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Simple chain creation failed: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test function\n",
    "def test_chain_creation():\n",
    "    \"\"\"Test the chain creation with sample data\"\"\"\n",
    "    print(\"=== TESTING CHAIN CREATION ===\")\n",
    "    \n",
    "    sample_pdf = pdf_text\n",
    "    \n",
    "    sample_prompt = \"You are an AI research assistant. Analyze academic papers and answer questions about them accurately.\"\n",
    "    \n",
    "    result = create_cached_chain(sample_pdf, sample_prompt)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"‚ùå Chain creation completely failed\")\n",
    "        return False\n",
    "    \n",
    "    chain, cached_content = result\n",
    "    cache_status = \"with cache\" if cached_content else \"without cache\"\n",
    "    print(f\"‚úÖ Chain created {cache_status}\")\n",
    "    \n",
    "    # Test the chain\n",
    "    try:\n",
    "        print(\"üîÑ Testing chain...\")\n",
    "        response = chain.invoke({\"input\": \"What is the main topic of this paper?\"})\n",
    "        print(f\"‚úÖ Chain test successful!\")\n",
    "        print(f\"Response: {response.content[:200]}...\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Chain test failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Quick environment check\n",
    "def check_setup():\n",
    "    \"\"\"Quick check of your setup\"\"\"\n",
    "    print(\"=== ENVIRONMENT CHECK ===\")\n",
    "    \n",
    "    # Check API key\n",
    "    if os.getenv(\"GOOGLE_API_KEY\"):\n",
    "        print(\"‚úÖ GOOGLE_API_KEY found\")\n",
    "    else:\n",
    "        print(\"‚ùå GOOGLE_API_KEY missing\")\n",
    "        print(\"   Set it with: export GOOGLE_API_KEY='your-key-here'\")\n",
    "        return False\n",
    "    \n",
    "    # Test Google AI connection\n",
    "    try:\n",
    "        genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "        models = list(genai.list_models())\n",
    "        print(f\"‚úÖ Connected to Google AI ({len(models)} models available)\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Google AI connection failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Run this to debug your issue\n",
    "if __name__ == \"__main__\":\n",
    "    if check_setup():\n",
    "        test_chain_creation()\n",
    "    else:\n",
    "        print(\"\\nüîß Fix the environment issues above first!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55d8fcc-ad07-48ec-98ee-36e6c1f374ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
